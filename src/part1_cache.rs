// Part 1: Hotel Availability Cache Implementation
// This component serves as the middleware between our high-traffic customer-facing API and supplier systems

use std::collections::HashMap;
use std::sync::atomic::{AtomicU64, AtomicUsize, Ordering};
use std::sync::{Arc, Mutex};
use std::time::{Duration, Instant};

// Enhanced stats for the cache
#[derive(Debug, Default)]
pub struct CacheStats {
    pub size_bytes: AtomicUsize,
    pub items_count: AtomicUsize,
    pub hit_count: AtomicUsize,
    pub miss_count: AtomicUsize,
    pub eviction_count: AtomicUsize,
    pub expired_count: AtomicUsize,
    pub rejected_count: AtomicUsize,
    pub average_lookup_time_ns: AtomicU64,
    pub total_lookups: AtomicUsize,
}

// Enhanced stats for the cache
#[derive(Debug, Default, Clone)]
pub struct CacheStatsReport {
    pub size_bytes: usize,
    pub items_count: usize,
    pub hit_count: usize,
    pub miss_count: usize,
    pub eviction_count: usize,
    pub expired_count: usize,
    pub rejected_count: usize,
    pub average_lookup_time_ns: u64,
    pub total_lookups: usize,
}

// Cache configuration options
#[derive(Debug, Clone)]
pub struct CacheConfig {
    pub max_size_mb: usize,
    pub default_ttl_seconds: u64,
    pub cleanup_interval_seconds: u64,
    pub shards_count: usize,
    pub eviction_policy: EvictionPolicy,
}

impl Default for CacheConfig {
    fn default() -> Self {
        Self {
            max_size_mb: 100,
            default_ttl_seconds: 300,
            cleanup_interval_seconds: 60,
            shards_count: 16,
            eviction_policy: EvictionPolicy::LeastRecentlyUsed,
        }
    }
}

// Eviction policy to use
#[derive(Debug, Clone, Copy, PartialEq)]
pub enum EvictionPolicy {
    LeastRecentlyUsed,
    LeastFrequentlyUsed,
    TimeToLive,
}

// Cache trait to implement with enhanced requirements
pub trait AvailabilityCache: Send + Sync + 'static {
    // Initialize a new cache with the given configuration
    fn new(config: CacheConfig) -> Self
    where
        Self: Sized;

    // Store availability data for a hotel on specific dates
    // TTL specifies how long the item should remain in the cache (None uses default from config)
    // Returns true if stored successfully, false if rejected (e.g., capacity limits)
    fn store(
        &self,
        hotel_id: &str,
        check_in: &str,
        check_out: &str,
        data: Vec<u8>,
        ttl: Option<Duration>,
    ) -> bool;

    // Retrieve availability data if it exists and is not expired
    // The bool in the tuple indicates if this was a cache hit
    fn get(&self, hotel_id: &str, check_in: &str, check_out: &str) -> Option<(Vec<u8>, bool)>;

    // Get cache statistics
    fn stats(&self) -> CacheStatsReport;

    // Set the eviction policy to use
    fn set_eviction_policy(&self, policy: EvictionPolicy);

    // Prefetch data for given keys - useful for warming the cache ahead of expected demand
    fn prefetch(&self, keys: Vec<(String, String, String)>, ttl: Option<Duration>) -> usize;

    // Bulk invalidate entries matching a pattern
    // For example, invalidate all entries for a specific hotel
    fn invalidate(
        &self,
        hotel_id: Option<&str>,
        check_in: Option<&str>,
        check_out: Option<&str>,
    ) -> usize;

    // Resize the cache (this might drop items if downsizing)
    fn resize(&self, new_max_size_mb: usize) -> bool;
}

// Helper function to create a cache key (you may modify this as needed)
pub fn create_cache_key(hotel_id: &str, check_in: &str, check_out: &str) -> String {
    format!("{}:{}:{}", hotel_id, check_in, check_out)
}

// Optional: Helper for calculating item size - implement if useful for your solution
pub fn calculate_item_size(key: &str, data: &[u8]) -> usize {
    key.len() + data.len() + std::mem::size_of::<Instant>() // Add more fields as needed for your implementation
}

pub struct ExampleCache {
    cache: Arc<Mutex<HashMap<String, CacheEntry>>>,
    config: Arc<Mutex<CacheConfig>>,
    stats: CacheStats,
}

struct CacheEntry {
    data: Vec<u8>,
    created_at: Instant,
    ttl: Duration,
    access_count: usize,
    last_accessed: Instant,
}

impl CacheEntry {
    fn is_expired(&self) -> bool {
        self.created_at.elapsed() > self.ttl
    }
}

impl ExampleCache {
    fn remove_oldest_entry(&self) {
        let cache = self.cache.lock().unwrap();
        let policy = self.config.lock().unwrap().eviction_policy;

        let oldest_key = match policy {
            EvictionPolicy::LeastRecentlyUsed => cache
                .iter()
                .min_by(|stat1, stat2| stat1.1.access_count.cmp(&stat2.1.access_count))
                .map(|(k, _)| k.clone()),
            EvictionPolicy::LeastFrequentlyUsed => cache
                .iter()
                .min_by(|stat1, stat2| stat1.1.last_accessed.cmp(&stat2.1.last_accessed))
                .map(|(k, _)| k.clone()),
            EvictionPolicy::TimeToLive => cache
                .iter()
                .min_by(|stat1, stat2| stat1.1.created_at.cmp(&stat2.1.created_at))
                .map(|(k, _)| k.clone()),
        };
        drop(cache);

        if let Some(oldest_key) = oldest_key {
            self.remove_entry(oldest_key, false);
        }
    }

    fn remove_entry(&self, key: String, expired: bool) {
        let mut cache = self.cache.lock().unwrap();
        if let Some(removed_data) = cache.remove(&key) {
            self.stats.size_bytes.fetch_sub(
                calculate_item_size(&key, &removed_data.data),
                Ordering::SeqCst,
            );
            self.stats.eviction_count.fetch_add(1, Ordering::SeqCst);
            self.stats.items_count.fetch_sub(1, Ordering::SeqCst);

            if expired {
                self.stats.expired_count.fetch_add(1, Ordering::SeqCst);
            }
        }
    }

    fn store_lookup_time(&self, now: Instant) {
        let duration_ns: u64 = now.elapsed().as_nanos() as u64;
        let total_lookups = self.stats.total_lookups.load(Ordering::SeqCst);
        let current_avg = self.stats.average_lookup_time_ns.load(Ordering::SeqCst);

        let new_avg = if total_lookups == 1 {
            duration_ns
        } else {
            (current_avg * (total_lookups as u64 - 1) + duration_ns) / (total_lookups as u64)
        };

        self.stats
            .average_lookup_time_ns
            .store(new_avg, Ordering::SeqCst);
    }
}

impl AvailabilityCache for ExampleCache {
    fn new(config: CacheConfig) -> Self {
        Self {
            cache: Arc::new(Mutex::new(HashMap::new())),
            config: Arc::new(Mutex::new(config)),
            stats: CacheStats::default(),
        }
    }

    fn store(
        &self,
        hotel_id: &str,
        check_in: &str,
        check_out: &str,
        data: Vec<u8>,
        ttl: Option<Duration>,
    ) -> bool {
        println!("Storing data for {} {}-{}", hotel_id, check_in, check_out);

        let default_ttl_seconds = self.config.lock().unwrap().default_ttl_seconds;
        let max_size_mb = self.config.lock().unwrap().max_size_mb;
        let key = create_cache_key(hotel_id, check_in, check_out);
        let ttl = ttl.unwrap_or_else(|| Duration::from_secs(default_ttl_seconds));

        // Simple size check (not perfect but demonstrates the concept)
        let item_size = calculate_item_size(&key, &data);
        let max_size_bytes = max_size_mb * 1024 * 1024;
        let current_size_bytes = self.stats.size_bytes.load(Ordering::SeqCst);

        if current_size_bytes + item_size > max_size_bytes {
            println!(
                "Cache size limit exceeded ({} + {} > {}), evicting oldest entry",
                current_size_bytes, item_size, max_size_bytes
            );
            self.remove_oldest_entry();
        }

        println!("Inserting item of size {} bytes into cache", item_size);

        let entry = CacheEntry {
            data,
            created_at: Instant::now(),
            ttl,
            access_count: 0,
            last_accessed: Instant::now(),
        };
        self.cache.lock().unwrap().insert(key.clone(), entry);
        self.stats.items_count.fetch_add(1, Ordering::SeqCst);
        self.stats.size_bytes.fetch_add(item_size, Ordering::SeqCst);

        true
    }

    fn get(&self, hotel_id: &str, check_in: &str, check_out: &str) -> Option<(Vec<u8>, bool)> {
        let now = Instant::now();
        let key = create_cache_key(hotel_id, check_in, check_out);

        self.stats.total_lookups.fetch_add(1, Ordering::SeqCst);

        let mut cache = self.cache.lock().unwrap();
        if let Some(entry) = cache.get_mut(&key) {
            if entry.is_expired() {
                drop(cache); // Release lock before calling remove_entry
                self.remove_entry(key, true);
                self.store_lookup_time(now);
                return None;
            }

            entry.access_count += 1;
            entry.last_accessed = Instant::now();
            self.stats.hit_count.fetch_add(1, Ordering::SeqCst);
            self.store_lookup_time(now);
            Some((entry.data.clone(), true))
        } else {
            self.stats.miss_count.fetch_add(1, Ordering::SeqCst);
            self.store_lookup_time(now);
            None
        }
    }

    fn stats(&self) -> CacheStatsReport {
        CacheStatsReport {
            size_bytes: self.stats.size_bytes.load(Ordering::SeqCst),
            items_count: self.stats.items_count.load(Ordering::SeqCst),
            hit_count: self.stats.hit_count.load(Ordering::SeqCst),
            miss_count: self.stats.miss_count.load(Ordering::SeqCst),
            eviction_count: self.stats.eviction_count.load(Ordering::SeqCst),
            expired_count: self.stats.expired_count.load(Ordering::SeqCst),
            rejected_count: self.stats.rejected_count.load(Ordering::SeqCst),
            average_lookup_time_ns: self.stats.average_lookup_time_ns.load(Ordering::SeqCst),
            total_lookups: self.stats.total_lookups.load(Ordering::SeqCst),
        }
    }

    fn set_eviction_policy(&self, policy: EvictionPolicy) {
        let mut config = self.config.lock().unwrap();
        config.eviction_policy = policy;
    }

    fn prefetch(&self, keys: Vec<(String, String, String)>, ttl: Option<Duration>) -> usize {
        // Simple implementation - in real system this would trigger backend calls
        let mut count = 0;
        for (hotel_id, check_in, check_out) in keys {
            // Simulate fetching data
            let dummy_data = vec![1, 2, 3, 4, 5];
            if self.store(&hotel_id, &check_in, &check_out, dummy_data, ttl) {
                count += 1;
            }
        }
        count
    }

    fn invalidate(
        &self,
        hotel_id: Option<&str>,
        check_in: Option<&str>,
        check_out: Option<&str>,
    ) -> usize {
        let cache = self.cache.lock().unwrap();
        let keys_to_remove: Vec<String> = cache
            .keys()
            .filter(|key| {
                let parts: Vec<&str> = key.split(':').collect();
                if parts.len() != 3 {
                    return false;
                }

                let matches_hotel = hotel_id.map_or(true, |h| parts[0] == h);
                let matches_checkin = check_in.map_or(true, |c| parts[1] == c);
                let matches_checkout = check_out.map_or(true, |c| parts[2] == c);

                matches_hotel && matches_checkin && matches_checkout
            })
            .cloned()
            .collect();
        drop(cache); // Release lock before removing entries

        let count = keys_to_remove.len();
        for key in keys_to_remove {
            self.remove_entry(key, false);
        }
        count
    }

    fn resize(&self, new_max_size_mb: usize) -> bool {
        self.config.lock().unwrap().max_size_mb = new_max_size_mb;

        let current_size_bytes = self.stats.size_bytes.load(Ordering::SeqCst);
        let new_max_size_bytes = new_max_size_mb * 1024 * 1024;

        if current_size_bytes > new_max_size_bytes {
            while self.stats.size_bytes.load(Ordering::SeqCst) > new_max_size_bytes {
                self.remove_oldest_entry();
            }
        }

        true
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::sync::Arc;
    use std::thread;
    use std::time::Duration;

    // Example of a more complex test for cache behavior under contention
    #[test]
    fn test_concurrent_access_with_contention() {
        let config = CacheConfig {
            max_size_mb: 5,
            default_ttl_seconds: 300,
            cleanup_interval_seconds: 60,
            shards_count: 8,
            eviction_policy: EvictionPolicy::LeastFrequentlyUsed,
        };

        println!("Starting contention test with config: {:?}", config);

        let cache = Arc::new(ExampleCache::new(config));
        let threads_count = 10; // High number of threads to create contention
        let operations_per_thread = 1000; // Number of operations per thread

        // Generate some popular keys that will have contention
        let popular_hotels = vec!["hotel1", "hotel2", "hotel3"];
        let popular_dates = vec![("2025-06-01", "2025-06-05"), ("2025-07-01", "2025-07-10")];

        // Pre-populate cache with some data
        for hotel in &popular_hotels {
            for (check_in, check_out) in &popular_dates {
                let data = vec![1, 2, 3, 4, 5]; // Example data
                println!(
                    "Pre-populating cache for {} {}-{}",
                    hotel, check_in, check_out
                );
                cache.store(hotel, check_in, check_out, data, None);
            }
        }

        println!("Pre-populated cache with popular keys.");

        let mut handles = vec![];
        for i in 0..threads_count {
            let cache_clone = Arc::clone(&cache);
            let popular_hotels = popular_hotels.clone();
            let popular_dates = popular_dates.clone();

            let handle = thread::spawn(move || {
                for j in 0..operations_per_thread {
                    // 80% of operations target popular items (creating contention)
                    let use_popular = rand::random::<f64>() < 0.8;

                    let hotel_id;
                    let check_in;
                    let check_out;

                    if use_popular {
                        // Use a popular hotel/date combination
                        hotel_id = popular_hotels[j % popular_hotels.len()].to_string();
                        let date_pair = &popular_dates[j % popular_dates.len()];
                        check_in = date_pair.0.to_string();
                        check_out = date_pair.1.to_string();
                    } else {
                        // Use a unique hotel/date combination
                        hotel_id = format!("hotel{}", i * 1000 + j);
                        check_in = format!("2025-{:02}-01", (j % 12) + 1);
                        check_out = format!("2025-{:02}-10", (j % 12) + 1);
                    }

                    // Mix of read-heavy operations
                    if j % 10 < 8 {
                        // 80% reads
                        println!(
                            "Thread {} [{}] performing get for {} {}-{}",
                            i, j, hotel_id, check_in, check_out
                        );
                        let _ = cache_clone.get(&hotel_id, &check_in, &check_out);
                    } else if j % 10 < 9 {
                        // 10% writes
                        println!(
                            "Thread {} [{}] performing store for {} {}-{}",
                            i, j, hotel_id, check_in, check_out
                        );
                        let data = vec![i as u8, j as u8, 1, 2, 3, 4, 5];
                        cache_clone.store(&hotel_id, &check_in, &check_out, data, None);
                    } else {
                        println!(
                            "Thread {} [{}] performing invalidate for {}",
                            i, j, hotel_id
                        );
                        // 10% invalidations
                        cache_clone.invalidate(Some(&hotel_id), None, None);
                    }
                }
            });

            handles.push(handle);
        }

        // Wait for all threads to complete
        for handle in handles {
            handle.join().unwrap();
        }

        // Check cache stats
        let stats = cache.stats();
        println!("Cache stats after contention test: {:?}", stats);

        // Verify average lookup time is reasonable
        assert!(
            stats.average_lookup_time_ns < 1_000_000, // 1ms
            "Average lookup time too high: {}ns",
            stats.average_lookup_time_ns
        );
    }

    #[test]
    fn test_expiration_and_ttl() {
        let config = CacheConfig {
            max_size_mb: 5,
            default_ttl_seconds: 5, // Short TTL for testing
            cleanup_interval_seconds: 1,
            shards_count: 4,
            eviction_policy: EvictionPolicy::LeastRecentlyUsed,
        };

        let cache = ExampleCache::new(config);

        let hotel_id = "hotel123";
        let check_in = "2025-06-01";
        let check_out = "2025-06-05";
        let data = vec![1, 2, 3, 4, 5];

        // Store with default TTL
        assert!(cache.store(hotel_id, check_in, check_out, data.clone(), None));

        // Store with custom shorter TTL
        let hotel_id2 = "hotel456";
        assert!(cache.store(
            hotel_id2,
            check_in,
            check_out,
            data.clone(),
            Some(Duration::from_secs(2))
        ));

        // Verify both are initially available
        assert!(cache.get(hotel_id, check_in, check_out).is_some());
        assert!(cache.get(hotel_id2, check_in, check_out).is_some());

        // Wait for the shorter TTL to expire
        thread::sleep(Duration::from_secs(3));

        // hotel_id2 should be expired, hotel_id should still be valid
        assert!(cache.get(hotel_id, check_in, check_out).is_some());
        assert!(cache.get(hotel_id2, check_in, check_out).is_none());

        // Wait for the longer TTL to expire
        thread::sleep(Duration::from_secs(3));

        // Now both should be expired
        assert!(cache.get(hotel_id, check_in, check_out).is_none());
        assert!(cache.get(hotel_id2, check_in, check_out).is_none());

        // Check expiration stats
        let stats = cache.stats();
        assert!(
            stats.expired_count >= 2,
            "Expected at least 2 expired items"
        );
    }

    #[test]
    fn test_eviction_policy_lru() {
        let config = CacheConfig {
            max_size_mb: 1, // Small size to force evictions
            default_ttl_seconds: 3600,
            cleanup_interval_seconds: 60,
            shards_count: 2,
            eviction_policy: EvictionPolicy::LeastRecentlyUsed,
        };

        let cache = ExampleCache::new(config);
        cache.set_eviction_policy(EvictionPolicy::LeastRecentlyUsed);

        // Fill cache with items
        let large_data = vec![0; 250 * 1024]; // 250KB items

        // Add 4 items totaling ~1MB to fill the cache
        for i in 0..4 {
            let hotel_id = format!("hotel{}", i);
            assert!(cache.store(
                &hotel_id,
                "2025-06-01",
                "2025-06-05",
                large_data.clone(),
                None
            ));
        }

        // Access item 0 and 2 to make them recently used
        assert!(cache.get("hotel0", "2025-06-01", "2025-06-05").is_some());
        assert!(cache.get("hotel2", "2025-06-01", "2025-06-05").is_some());

        // Add another item, which should evict least recently used (hotel1 or hotel3)
        assert!(cache.store(
            "hotel4",
            "2025-06-01",
            "2025-06-05",
            large_data.clone(),
            None
        ));

        // hotel0 and hotel2 should still be in cache
        assert!(cache.get("hotel0", "2025-06-01", "2025-06-05").is_some());
        assert!(cache.get("hotel2", "2025-06-01", "2025-06-05").is_some());

        // Either hotel1 or hotel3 should be evicted
        let evicted = cache.get("hotel1", "2025-06-01", "2025-06-05").is_none()
            || cache.get("hotel3", "2025-06-01", "2025-06-05").is_none();
        assert!(evicted, "Expected LRU eviction to remove hotel1 or hotel3");

        // Verify eviction stats
        let stats = cache.stats();
        assert!(stats.eviction_count > 0, "Expected evictions to occur");
    }

    #[test]
    fn test_prefetch_and_invalidate() {
        let config = CacheConfig::default();
        let cache = ExampleCache::new(config);

        // Define some keys to prefetch
        let keys = vec![
            (
                "hotel1".to_string(),
                "2025-06-01".to_string(),
                "2025-06-05".to_string(),
            ),
            (
                "hotel1".to_string(),
                "2025-06-10".to_string(),
                "2025-06-15".to_string(),
            ),
            (
                "hotel2".to_string(),
                "2025-06-01".to_string(),
                "2025-06-05".to_string(),
            ),
        ];

        // This would trigger backend calls in a real implementation
        // We'll simulate it by pre-populating the cache
        for (hotel, check_in, check_out) in &keys {
            let data = vec![1, 2, 3, 4, 5];
            cache.store(hotel, check_in, check_out, data, None);
        }

        // Test bulk invalidation for a specific hotel
        let invalidated = cache.invalidate(Some("hotel1"), None, None);
        assert_eq!(invalidated, 2, "Expected 2 items to be invalidated");

        // Verify hotel1 entries are gone
        assert!(cache.get("hotel1", "2025-06-01", "2025-06-05").is_none());
        assert!(cache.get("hotel1", "2025-06-10", "2025-06-15").is_none());

        // But hotel2 entry should still be there
        assert!(cache.get("hotel2", "2025-06-01", "2025-06-05").is_some());

        // Test prefetching (would trigger backend calls in real impl)
        let prefetched = cache.prefetch(keys, None);
        assert_eq!(prefetched, 3, "Expected 3 items to be prefetched");

        // All items should be in cache now
        assert!(cache.get("hotel1", "2025-06-01", "2025-06-05").is_some());
        assert!(cache.get("hotel1", "2025-06-10", "2025-06-15").is_some());
        assert!(cache.get("hotel2", "2025-06-01", "2025-06-05").is_some());
    }

    #[test]
    fn test_cache_resize() {
        let config = CacheConfig {
            max_size_mb: 10,
            default_ttl_seconds: 300,
            cleanup_interval_seconds: 60,
            shards_count: 4,
            eviction_policy: EvictionPolicy::LeastRecentlyUsed,
        };

        let cache = ExampleCache::new(config);

        // Add some data
        let medium_data = vec![0; 100 * 1024]; // 100KB
        for i in 0..50 {
            let hotel_id = format!("hotel{}", i);
            cache.store(
                &hotel_id,
                "2025-06-01",
                "2025-06-05",
                medium_data.clone(),
                None,
            );
        }

        // Resize to smaller capacity
        println!("Resizing cache to smaller capacity");
        assert!(cache.resize(2));

        // Cache should evict items to maintain size limit
        let stats = cache.stats();
        assert!(
            stats.size_bytes <= 2 * 1024 * 1024,
            "Cache size exceeds 2MB after resizing: {}",
            stats.size_bytes
        );
        assert!(
            stats.items_count < 50,
            "Expected some items to be evicted after resizing"
        );

        // Resize to larger capacity
        println!("Resizing cache to larger capacity");
        assert!(cache.resize(20));

        // Add more data
        for i in 50..150 {
            let hotel_id = format!("hotel{}", i);
            cache.store(
                &hotel_id,
                "2025-06-01",
                "2025-06-05",
                medium_data.clone(),
                None,
            );
        }

        // Cache should accommodate the data
        let new_stats = cache.stats();
        assert!(
            new_stats.items_count > stats.items_count,
            "Cache should accommodate more items after upsizing"
        );
    }
}
